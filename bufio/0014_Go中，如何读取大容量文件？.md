如何在 Go 语言环境中读取**超大文件**？如果把这道题当做是一个面试题，怎么处理？

为什么有这个问题？很直观的，如果是超大文件，自然是那些 10GB、100GB 体量的文件，如何对这些文件读取并出处理其中的内容？

这个现实问题，可以扩展成：**读取文件有哪些方法？如果文件容量较大，如何改进读取方法？**

# 1 读取文件的方法

既然是想读取文件，那大致流程应该是：

1. 打开文件系统中的指定文件，获取 `*File` 实例；
2. `*File` 实例实现了 `io.Reader` 接口，可以通过这个接口读取文件的内容。

大致的过程都是如此，相当于把文件系统上的文件当做是**一种流式数据**进行处理。

那下面我们具体看看 Go 中的标准库提供了哪些方法实现文件读取。

# 2 超大文件？

在 io/ioutil 包中（实现有一些 I/O 工具函数），有这样的函数：

`func ReadFile(filename string) ([]byte, error)`：从 filename 中**读取整个文件**。成功地读取时，err 值为 nil，而不是 io.EOF。因为 ReadFile 方法会读取整个文件后返回，对于文件的 io.EOF 并不会当作是 error 报告。

~~~go
package main

import (
	"fmt"
	"io/ioutil"
	"log"
	"os"
)

func main() {
	file, err := os.Open("./ubuntu-16.04.6-desktop-i386.iso")
	if err != nil {
		log.Fatal(err.Error())
	}
	fmt.Println("Open File success!", file.Name())

	info, err := file.Stat()
	if err != nil {
		log.Fatal(err.Error())
	}

	size := float32(info.Size()) / 1024 / 1024 / 1024
	fmt.Println("Get file info success!", size, ("GB"), "<--", info.Size())

	// 获取 File 实例，且其实现了 io.Reader 接口
	fmt.Println("Start to read File!")
	contentBytes, err := ioutil.ReadFile(file.Name())
	if err != nil {
		log.Fatal(err.Error())
	}
	fmt.Println("Read End!", len(contentBytes))
}
Open File success! ./ubuntu-16.04.6-desktop-i386.iso
Get file info success! 1.5625 GB <-- 1677721600
Start to read File!
Read End! 1677721600
~~~

在 Go 1.16 中，可使用 os 包下的函数 `func ReadFile(name string) ([]byte, error)` 替换！

我们来看看运行时进程的具体情况：

![](./img/Snipaste_2021-07-01_14-43-33.png)

如果是从加载了整个文件的角度出发，上图的内存占用确实验证了这个过程。VSCode 进程占用内存：2,040.9MB，其中就包括 file 1.5625 GB 整个文件。进程结束后，内存占用率迅速降低到正常水平。

我们来看看其底层实现：

~~~go
func ReadFile(filename string) ([]byte, error) {
	f, err := os.Open(filename)
	...
	return readAll(f, n)
}

func readAll(r io.Reader, capacity int64) (b []byte, err error) {
	var buf bytes.Buffer
	...
	_, err = buf.ReadFrom(r)
	return buf.Bytes(), err
}

// ReadFrom reads data from r until EOF and appends it to the buffer, growing
// the buffer as needed. The return value n is the number of bytes read. Any
// error except io.EOF encountered during the read is also returned. If the
// buffer becomes too large, ReadFrom will panic with ErrTooLarge.
func (b *Buffer) ReadFrom(r io.Reader) (n int64, err error) {
	b.lastRead = opInvalid
	for {
		i := b.grow(MinRead)
		b.buf = b.buf[:i]
		m, e := r.Read(b.buf[i:cap(b.buf)])
		if m < 0 {
			panic(errNegativeRead)
		}

		b.buf = b.buf[:i+m]
		n += int64(m)
		if e == io.EOF {
			return n, nil // e is EOF, so return nil explicitly
		}
		if e != nil {
			return n, e
		}
	}
}
~~~

ReadFile 调用了 readAll，进而调用了 bytes.Buffer.ReadFrom(r io.Reader)，在其内部使用 for 循环持续读取到 io.EOF，也就是**读取了整个文件**。bytes.Buffer 的中的 []byte 就是读取到字节内容，[]byte 的长度就是整个文件的字节数。**可想而知，这个 []byte 占用了如此大的内存空间！**

从源代码来看，实际上是**因为 `*File` 实现了 `io.Reader` 接口**，也就是说能够通过调用 `Read(p []byte) (n int, err error)` 方法实现文件的读取。既然如此，**我们就可以变更这个过程，以便减少内存占用**。

比如：（如果是**文本文件**）**逐行读取**，也就是说每次仅读取一行：

~~~go
...
bufReader := bufio.NewReader(file)
for {
	line, _, err := bufReader.ReadLine()
	if err != nil {
		log.Fatal(err.Error())
		break
	}
	fmt.Println(string(line))
}
...
~~~

要知道 Go 标准包：

* bytes：处理和 []byte 相关的工具函数，而且包装了 bytes.Buffer 类型，可用于读取文件**（整体读取）**。
* bufio：和 bytes 最大的区别在于，其中包装了 bufio.Reader、bufio.Scanner 以及 bufio.Writer，可用于**缓冲式的 IO 操作**。

作为上述逐行读取的替换方案，也可以使用 bufio.Scanner 作为辅助：

~~~go
...
scanner := bufio.NewScanner(file)
for scanner.Scan() {
	line := scanner.Text()
	fmt.Println(line)
}

err = scanner.Err()
if err != nil {
	log.Fatal(err.Error())
}
...
~~~

使用 Scanner 的原因：bufio.Scanner 特别适用于**有换行符分隔的文本文件**，默认会在读取了 io.Reader 后，将输入拆分成一行行的格式。也就是默认的拆分标识就是**换行符**。

再比如：**分块读取**，也就是按照固定大小的块读取文件。自然的，需要创建一个大小为 chunkSize 的字节缓冲区，作为读取到的内容的临时存储地。

~~~go
...
buf = make([]byte, 200) // 如果对于较大文件，设置成 4096 比较合适，即 4KB 缓冲器

for {
	n, err := file.Read(buf)
	if err != nil {
		if err == io.EOF {
			break
		}
		log.Fatal(err.Error())
	}
	fmt.Println(n)
}
...
~~~

在 for 循环中，顺次读取 file 中的字节流数据，每次读到的内容不会超出 buf 的容量。也就是说，最大单次内存占用不会超出 4KB，

# 3 后续

开篇我们提出了一个问题：如何在 Go 语言环境中读取**超大文件**？

由这个问题，引出了如何使用 Go 读取文件，进而引出了标准库中用于读取文件的一系列类型、方法和函数，比如：bytes.Buffer、bufio.Reader、bufio.Scanner 等。

从上述读取方法的实践中，发现一些方法是将文件整体读取到内存中，也就是进程占用了大量的内存，这在读取大容量文件时显然是不可取的。**能够一次性全部读取到内存中吗？**肯定是不行的，因为内存是有限制的，而且会引起内存的较大波动。
