第一次请求时将一些**耗时操作的结果**暂存（存），以后遇到**相同的请求**（比较），（取）直接**返回暂存的数据**。整个过程涉及到 3 个方面动作：存、比较和取。

在计算机系统中，缓存无处不在。比如我们访问一个网页，网页和引用的 JS/CSS 等静态文件，根据不同的策略，会缓存在**浏览器本地**或是 **CDN 服务器**，那在第二次访问的时候，就会觉得网页加载的速度快了不少。再比如，微博的点赞数量，不可能每个客户端的每次访问，都从数据库中查找所有点赞的记录再做统计，**数据库的操作是很耗时的**，难以支持这么大的流量。所以，一般点赞这类数据是缓存在 Redis 服务器集群中的。

> 浏览器，是属于本地运行的服务；CDN 服务器则是对应服务端的服务。

**商业世界**里，**现金**为王；**架构世界**里，**缓存**为王！

**缓存**中最简单的莫过于**存储在内存中**的键值对缓存了。键值对，在 Go 中称之为 map。那直接创建一个 map，每次有新数据就往 map 中插入，这样做有什么问题？

1. **内存不够了怎么办？**

   向 map 中插入数据，特别是在考虑 map 底层数据结构时，插入数据多了后由于 Hash 碰撞，导致性能急剧下降（map的原理到底是怎样的？）。那就删除几条数据，是随机删除呢？还是按照时间顺序删除？有没有其他更好的缓冲淘汰策略呢？不同**数据访问频率**是不一样的，优先删除访问频率低的数据是不是更好？**数据的访问频率**可能随着时间变化，那优先删除**最近最少访问**的数据可能是一个更好的选择。

2. **并发写入冲突了怎么办？**

   **对缓存的访问，一般不可能是串行的**。map 是没有并发保护的，应对并发的场景需要加锁。

3. **单机性能不够怎么办？**

   单台计算机的资源是有限的，**计算、存储（指内存）**等都是有限的。

   随着业务量和访问量的增加，单台机器很容易遇到瓶颈。如果**利用多台计算机的资源**，**并行处理**提高性能就要缓存应用能够**支持分布式**，这称为**水平扩展**。与水平扩展相对应的是**垂直扩展**，即通过**增加单个节点的计算、存储、带宽**等，来提高系统的性能，硬件的成本和性能并非呈线性关系，大部分情况下，分布式系统是一个更优的选择。

4. 等等

设计一个**分布式缓存系统**，需要考虑资源控制、淘汰策略、并发、分布式节点通信等各个方面的问题。而且，**针对不同的应用场景，还需要在不同的特性之间权衡**，例如，是否需要支持缓存更新？还是假定缓存在淘汰之前是不允许改变的。不同的权衡对应着不同的实现。接下来的内容是**模仿 [groupcache](https://github.com/golang/groupcache) 的实现**，裁剪了部分功能。但总体实现上，还是与 groupcache 非常接近的。支持特性有：

- 单机缓存和基于 HTTP 的分布式缓存
- 最近最少访问(Least Recently Used, LRU) 缓存策略
- 使用 Go 锁机制防止缓存击穿
- 使用一致性哈希选择节点，实现负载均衡
- 使用 protobuf 优化节点间二进制通信
- ...

接下来，我们就开始设计和实现一个**分布式缓存系统**！

# 1 LRU 缓存淘汰策略

分布式缓存系统中的**所有缓存都存储在内存中**（如果不是存在内存中，那就不能称之为缓存），内存是有限的，由此不可能无限制地添加数据。

假定我们设置缓存能够使用的内存大小为 N，那么在添加了某一条缓存记录之后，占用内存超过了 N（阈值），这个时候就需要从缓存中移除一条或多条数据了。那问题在于移除谁呢？我们肯定希望尽可能移除“没用”的数据，那**如何判定数据“有用”或“没用”呢**？

> 也就是，使用**量化关系**，将每个数据的特征进行量化，通过**比较**量化后的值，达到判定数据“有用”的目的。

* **First In First Out**

  意为：**先进先出**，也就是淘汰最早添加的记录。FIFO 认为，最早添加的记录，其不再被使用的可能性比刚添加的可能性大。这种算法的实现非常简单：创建一个队列，新增记录（**判断何时需要新增记录，比如先在缓存中查找，但未找到**）添加到**队尾**，每次内存不够时，淘汰**队首**（仅考虑的是记录被添加进来的时间）。但是很多场景下，部分记录虽然是最早添加但也最常被访问，而不得不因为待的时间太长而**被淘汰**。这类数据会被频繁地添加进缓存，又被淘汰出去，导致**缓存命中率**降低。

* **Least Frequently Used**

  最少使用，也就是淘汰缓存中给**访问频率最低的记录**。**LFU 认为，如果数据过去被访问多次，那么将来被访问的频率也更高**。LFU 的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排列，淘汰时选择访问次数最少的即可。LFU 算法的命中率是比较高的，但缺点也非常明显，**维护每个记录的访问次数**，对内存的消耗是很高的；另外，如果数据的**访问模式**发生变化，LFU 需要较长的时间去适应，也就是说 LFU 算法受历史数据的影响比较大。例如某个数据历史上访问次数**奇高**，但在某个时间点之后几乎不再被访问，但因为历史访问次数过高，而迟迟不能被淘汰。

* **Least Recently Used**

  最近最少使用，相对于仅考虑时间因素的 FIFO 和仅考虑访问频率的 LFU，LRU 算法可以认为是**相对平衡的一种淘汰算法**。**LRU 认为，如果数据最近被访问过，那么将来被访问的概率也会更高**。

  LRU 算法的实现非常简单，维护一个队列，如果某条记录被访问了，则移动到**队首**。那么，**队尾**是最近最少访问的数据，淘汰该条记录即可。

由此，需要实现的是这样的数据结构：

![](./img/doubly_linked_list.png)

这张图很好地表示了 LRU 算法**最核心的 2 个数据结构**：

- 绿色的是字典(map)，存储键和值的映射关系。这样根据某个键(key)查找对应的值(value)的复杂是`O(1)`，在字典中插入一条记录的复杂度也是`O(1)`。
- 红色的是双向链表(double linked list)实现的队列。将所有的值放到双向链表中，这样，当访问到某个值时，将其移动到队尾的复杂度是`O(1)`，在队尾新增一条记录以及删除一条记录的复杂度均为`O(1)`。

首先要构造数据结构：

~~~go
type Cache struct {
	maxBytes  int64                         // Cache最大能容纳字节数
	nbytes    int64                         // 当前已装载容量
	ll        *list.List                    // Cache数据结构中双端链表
	cache     map[string]*list.Element      // key-value map，用于找到指定key对应的 *list.Element
	onRemoved func(key string, value Value) // Callback 机制
}
~~~

其中 `Cache.cache` 是为了依据 key 在 `Cache.ll` 中找到对应的节点。

接下来实现 Cache 的功能：

~~~go
type entry struct {
	key   string
	value Value
}

type Value interface {
	Len() int
}

func New(maxBytes int64, onRemoved func(key string, value Value)) *Cache {
	return &Cache{
		maxBytes:  maxBytes,
		ll:        list.New(),
		cache:     make(map[string]*list.Element),
		onRemoved: onRemoved,
	}
}

func (c *Cache) Add(key string, value Value) {
	if ele, ok := c.cache[key]; ok { // ele 类型：*list.Element
		// update the exists Element
		c.ll.MoveToFront(ele)
		kv := ele.Value.(*entry) // ele.Value 类型：*entry
		c.nbytes += int64(value.Len()) - int64(kv.value.Len())
		kv.value = value
	} else {
		// add a new Element
		newNode := c.ll.PushFront(&entry{key, value})
		c.cache[key] = newNode
		c.nbytes += int64(len(key) + value.Len())
	}
	for c.maxBytes != 0 && c.nbytes > c.maxBytes {
		c.RemoveOldest()
	}
}

func (c *Cache) Get(key string) (value Value, ok bool) {
	if ele, ok := c.cache[key]; ok {
		// Cache最前端是最近一次访问的节点，末尾是最早访问的节点
		c.ll.MoveToFront(ele)
		kv := ele.Value.(*entry) // ele.Value 类型：*entry
		return kv.value, true    // 如果直接写成 return 返回的 ok 值为 false
	}
	return
}

func (c *Cache) RemoveOldest() {
	ele := c.ll.Back()
	if ele != nil {
		kv := ele.Value.(*entry)
		delete(c.cache, kv.key)
		c.ll.Remove(ele)
		c.nbytes -= (int64(len(kv.key)) + int64(kv.value.Len()))
		if c.onRemoved != nil {
			c.onRemoved(kv.key, kv.value)
		}
	}
}

func (c *Cache) Len() int64 {
	return int64(c.ll.Len())
}
~~~

使用了标准库中原生的 `list.List`——一种双端队列数据结构，其中每个节点的数据结构是 `list.Element`，其中元素中存放的值 Value 在上述 Cache 中是：entry 类型，是一种 `key-value` 值。

对应的测试代码：

~~~go
package lru

import (
	"reflect"
	"testing"
)

type String string

func (s String) Len() int {
	return len(s)
}

func TestGet(t *testing.T) {
	lru := New(int64(0), nil)
	lru.Add("key1", String("1234"))

	if v, ok := lru.Get("key1"); !ok || string(v.(String)) != "1234" {
		t.Log(ok, string(v.(String)))
		t.Fatal("cache hit key1=1234 failed")
	}
	if _, ok := lru.Get("key2"); ok {
		t.Fatal("cache miss key2 failed")
	}
}

func TestRemoveoldest(t *testing.T) {
	k1, k2, k3 := "key1", "key2", "key3"
	v1, v2, v3 := "value1", "value2", "value3"

	cap := len(k1 + k2 + v1 + v2)
	lru := New(int64(cap), nil)
	lru.Add(k1, String(v1))
	lru.Add(k2, String(v2))
	lru.Add(k3, String(v3))

	if _, ok := lru.Get("key1"); ok || lru.Len() != 2 {
		t.Fatal("Removeoldest key1 failed")
	}
}

func TestOnEvicted(t *testing.T) {
	keys := make([]string, 0)
	callback := func(key string, value Value) {
		keys = append(keys, key)
	}

	lru := New(int64(10), callback)
	lru.Add("key1", String("123456"))
	lru.Add("k2", String("k2"))
	lru.Add("k3", String("k3"))
	lru.Add("k4", String("k4"))

	expect := []string{"key1", "k2"}

	t.Log(keys)
	if !reflect.DeepEqual(expect, keys) {
		t.Fatalf("call onRemoved failed, expect keys equals to %s", expect)
	}
}
~~~

# 2 单机并发缓存

想要了解并发，可以从简单的例子出发：

~~~go
package main

import (
	"fmt"
	"time"
)

func main() {
	set := make(map[int]bool)

	for i := 0; i < 10; i++ {
		go func() {
			if _, ok := set[100]; !ok {
				fmt.Println("100")
				set[100] = true
			}
		}()
	}

	time.Sleep(1 * time.Second)
}
~~~

这段程序的核心数据变量是：set，是一个 `map[int]bool`。我们知道在 Go 中的 map 是**不支持并发访问**的，这段程序的输出有很多中形式，可能打印了 4 个 100，也可能打印了很多个，还有可能直接 crash——`fatal error: concurrent map read and map write`！

多个 goroutine **同时读写同一个变量**，在并发度较高的情况下，会发生**冲突**。确保一次只有一个 goroutine 可以访问该变量以避免冲突，这称之为`互斥`，互斥锁可以解决这个问题。

> sync.Mutex 是一个互斥锁，可以由**不同的** goroutine 加锁和解锁。

`sync.Mutex` 是 Go 语言标准库提供的一个互斥锁，当一个 goroutine 获得了这个锁的拥有权后，其它**请求锁**的 goroutine 就会阻塞在 `Lock()` 方法的调用上，直到调用 `Unlock()` 锁被释放。

经过 sync.Mutex 改造后的代码如下：

~~~go
package main

import (
	"fmt"
	"sync"
	"time"
)

func main() {
	set := make(map[int]bool)

	var lock sync.Mutex

	for i := 0; i < 10; i++ {
		go func() {
			lock.Lock()
			defer lock.Unlock()
			if _, ok := set[100]; !ok {
				fmt.Println("100")
				set[100] = true
			}
		}()
	}

	time.Sleep(1 * time.Second)
}
~~~

相同的数字只会被打印一次。当一个 goroutine 调用了 `Lock()` 方法时，**其他 goroutine 调用 `Lock()` 时会被阻塞**，直到 `Unlock()` 调用将锁释放。因此被包裹部分的代码就能够避免冲突，实现互斥。

在将支持并发的功能集成到项目前，封装表示缓存值的数据结构 ByteView：

~~~go
package v4

type ByteView struct {
	b []byte
}

// Len 实现 lru.go 中的 Value 接口
func (v ByteView) Len() int {
	return len(v.b)
}

// ByteSlice 做了一次深拷贝，防止反馈给用户后修改缓存值（Update入口做统一管理）
func (v ByteView) ByteSlice() []byte {
	return cloneBytes(v.b)
}

func (v ByteView) String() string {
	return string(v.b)
}

func cloneBytes(src []byte) []byte {
	//FIXME dest 如果没有初始化会出现问题，copy 将不会执行拷贝操作
	var dest []byte
	dest = make([]byte, len(src))
	copy(dest, src)
	return dest
}
~~~

ByteView 是一个**只读的数据结构**，用来表示缓存值。关于这个 ByteView 的设计主要考虑：

1. ByteView 的方法接收者是 ByteView 类型，而不是其指针类型，避免缓存值被修改；
2. ByteSlice 做了深拷贝，同样是为了避免修改；
3. ByteView 只有一个数据成员，`b []byte`，b 将会**存储真实的缓存值**。选择 byte 类型是为了能够支持任意的数据类型的存储，例如字符串、图片等。

接下来为 Cache 添加并发特性：

~~~go
package v4

import (
	"sync"

	"github.com/go-examples-with-tests/net/http/v4/lru"
)

type cache struct {
	lock       sync.Mutex // 无需初始化，直接就可使用
	lru        *lru.Cache
	cacheBytes int64
}

func (c *cache) add(key string, view ByteView) {
	c.lock.Lock()
	defer c.lock.Unlock()

	if c.lru == nil {
		c.lru = lru.New(c.cacheBytes, nil)
	}
	c.lru.Add(key, view)
}

func (c *cache) get(key string) (v ByteView, ok bool) {
	c.lock.Lock()
	defer c.lock.Unlock()
	if c.lru == nil {
		return
	}

	if value, ok := c.lru.Get(key); ok {
		return value.(ByteView), true // 返回了 ByteView 后，不会被改变吗？
	}
	return
}

~~~

在 `add` 方法中，判断了 `c.lru` 是否为 nil，如果等于 nil 再创建实例。这种方法称之为**延迟初始化**(Lazy Initialization)，一个对象的延迟初始化意味着该对象的创建将会延迟至第一次使用该对象时。这种做法，主要**用于提高性能，并减少程序内存要求**。

我们可以看到，cache 实际上进一步封装了 `*lru.Cache`，同时初始化了一个 sync.Mutex 实例用于并发访问。这种做法和工程基建类似，都是从“打地基”开始，一步步向上构建——**自底向上**。

接下来我们构建分布式缓存系统的**逻辑雏形**：

~~~go
                            是
接收 key --> 检查是否被缓存 -----> 返回缓存值 ⑴
                |  否                         是
                |-----> 是否应当从远程节点获取 -----> 与远程节点交互 --> 返回缓存值 ⑵
                            |  否
                            |-----> 调用`回调函数`，获取值并添加到缓存 --> 返回缓存值 ⑶
~~~

上面的步骤 1 和 3 是本节内容实现的部分，步骤 2 需要**让缓存系统支持分布式**。

整个代码结构如下：

~~~go
geecache/
    |--lru/
        |--lru.go  // lru 缓存淘汰策略
    |--byteview.go // 缓存值的抽象与封装
    |--cache.go    // 并发控制
    |--geecache.go // 负责与外部交互，控制缓存存储和获取的主流程
~~~

geocache.go 中封装的 GeeCache 数据结构是整个分布式缓存系统的**核心数据结构**，负责**与外部交互，并控制缓存存储和获取的主流程**。下面来实现主要逻辑：

~~~go
package v4

import (
	"fmt"
	"log"
	"sync"
)

type Group struct {
	name      string
	getter    Getter
	mainCache cache //FIXME 此处为什么不能是 *cache？什么时候使用指针，什么时候使用普通类型？
}

var (
	mu     sync.RWMutex
	groups = make(map[string]*Group) //FIXME 此处为什么存的是 *Group？
)

type Getter interface {
	Get(key string) ([]byte, error)
}

type GetterFunc func(key string) ([]byte, error) // 接口型函数，实现了Getter接口

func (f GetterFunc) Get(key string) ([]byte, error) {
	return f(key)
}

func NewGroup(name string, cacheBytes int64, getter Getter) *Group {
	if getter == nil {
		panic("getter is nil")
	}

	if _, exist := groups[name]; exist {
		panic("group " + name + " exists")
	}

	mu.Lock()
	defer mu.Unlock()
	g := &Group{
		name:      name,
		getter:    getter,
		mainCache: cache{cacheBytes: cacheBytes},
	}
	groups[name] = g
	return g
}

func GetGroup(name string) *Group {
    // 使用了只读锁 RLock()，因为不涉及任何冲突变量的写操作。
	mu.RLock()
	// defer mu.RUnlock()
	g := groups[name]
	mu.RUnlock()
	return g
}

func (g *Group) Get(key string) (ByteView, error) {
	if key == "" {
		return ByteView{}, fmt.Errorf("key is required")
	}
	if v, ok := g.mainCache.get(key); ok {
		log.Println("[GeeCache] hit")
		return v, nil
	}

	return g.load(key)
}

func (g *Group) load(key string) (ByteView, error) {
	// 调用 getter，用户自定义获取数据方式
	return g.getLocally(key)
}

func (g *Group) getLocally(key string) (ByteView, error) {
	// 调用 getter 从数据源获取数据
	bytes, err := g.getter.Get(key)
	if err != nil {
		return ByteView{}, err
	}
	value := ByteView{b: cloneBytes(bytes)}
	g.populateCache(key, value)
	return value, nil
}

func (g *Group) populateCache(key string, value ByteView) {
	g.mainCache.add(key, value)
}
~~~

如果缓存不存在，应从数据源（文件，数据库等）获取数据并添加到缓存中。GeeCache 是否应该支持多种数据源的配置呢？不应该，一是数据源的种类太多，没办法一一实现；二是扩展性不好。如何从源头获取数据，应该是用户决定的事情，我们就把这件事交给用户好了。因此，我们设计了一个**回调函数**(callback)，在缓存不存在时，调用这个函数，得到源数据。

- 定义接口 Getter 和 回调函数 `Get(key string)([]byte, error)`，参数是 key，返回值是 []byte。
- 定义函数类型 GetterFunc，并实现 Getter 接口的 `Get` 方法。
- 函数类型实现某一个接口，称之为**接口型函数**，方便使用者在调用时既能够**传入函数**作为参数，也能够**传入实现了该接口的结构体**作为参数。

定义一个函数类型 F，并且实现接口 A 的方法，然后在这个方法中**调用自己**。这是 Go 语言中将其他函数（参数返回值定义与 F 一致）转换为接口 A 的常用技巧。

代码测试：

~~~go
package v4

import (
	"fmt"
	"log"
	"reflect"
	"testing"
)

var db = map[string]string{
	"Tom":  "630",
	"Jack": "589",
	"Sam":  "567",
}

func TestGetter(t *testing.T) {
	var f Getter = GetterFunc(func(key string) ([]byte, error) { // 函数类型的初始化，并赋值给一个接口
		return []byte(key), nil
	})
	expect := []byte("key")
	if v, _ := f.Get("key"); !reflect.DeepEqual(v, expect) {
		t.Fatal("callback failed")
	}
}

func TestGroup(t *testing.T) {
	NewGroup("score", 2<<10, GetterFunc(func(key string) (bytes []byte, err error) {
		return
	}))

	if group := GetGroup("score"); group == nil || group.name != "score" {
		t.Fatal("create new group failed")
	}
	if group := GetGroup("score" + "xxx"); group != nil {
		t.Fatal("get a error group")
	}
}

func TestGet(t *testing.T) {
	loadCount := make(map[string]int)
	gee := NewGroup("school score", 2<<10, GetterFunc(func(key string) ([]byte, error) {
		log.Println("[SlowDB] search key:", key)
		if v, ok := db[key]; ok {
			if _, ok := loadCount[key]; !ok {
				loadCount[key] = 0
			}
			// 表示从db中加载数据的次数
			loadCount[key]++
			return []byte(v), nil
		}
		return nil, fmt.Errorf("%s not exist", key)
	}))
	for k, v := range db {
		if view, err := gee.Get(k); err != nil || view.String() != v {
			t.Log(err, view.String())
			t.Fatalf("failed to get value of %s", k)
		}
		if _, err := gee.Get(k); err != nil || loadCount[k] > 1 {
			t.Fatalf("cache %s miss", k)
		}
	}
	if view, err := gee.Get("unknown"); err == nil {
		t.Fatalf("the value of unknow should be empty, but %s got", view)
	}
}
~~~

# 3 HTTP 服务端

分布式缓存需要实现节点间通信，建立基于 HTTP  通信机制是比较常见和简单的做法。如果一个节点启动了 HTTP 服务，那么这个节点就可以被其他节点访问。下面的内容就是为单机节点搭建 HTTP Server：

~~~go
geecache/
    |--lru/
        |--lru.go  // lru 缓存淘汰策略
    |--byteview.go // 缓存值的抽象与封装
    |--cache.go    // 并发控制
    |--geecache.go // 负责与外部交互，控制缓存存储和获取的主流程
	|--http.go     // 提供被其他节点访问的能力(基于http)
~~~

创建一个结构体 HTTPPool，作为承载**节点间 HTTP 通信的核心数据结构**：

~~~go
const defaultBasePath = "/_geecache/"

// 承载节点 HTTP 通信的核心数据结构
type HTTPPool struct {
	self     string
	basePath string
}

func NewHTTPPool(self string) *HTTPPool {
	return &HTTPPool{
		self:     self,
		basePath: defaultBasePath,
	}
}
~~~

- `HTTPPool` 只有 2 个参数，一个是 self，用来记录本机地址，包括主机名/IP 和端口。
- 另一个是 basePath，作为节点间通讯地址的前缀，默认是 `/_geecache/`，那么 http://example.com/_geecache/ 开头的请求，就用于节点间的访问。因为一个主机上还可能承载其他的服务，加一段 Path 是一个好习惯。比如，大部分网站的 API 接口，一般以 `/api` 作为前缀。

实现**访问缓存**的方法：

~~~go
func (p *HTTPPool) Log(format string, v ...interface{}) {
	log.Printf("[Server %s] %s", p.self, fmt.Sprintf(format, v...))
}

func (p *HTTPPool) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	if !strings.HasPrefix(r.URL.Path, p.basePath) {
		panic("HTTPPool serving unexpected path: " + r.URL.Path)
	}
	p.Log("%s %s", r.Method, r.URL.Path)

	// /<basepath>/<groupname>/<key>
	parts := strings.SplitN(r.URL.Path[len(p.basePath):], "/", 2)
	if len(parts) != 2 {
		http.Error(w, "bad request", http.StatusBadRequest)
		return
	}

	groupName := parts[0]
	key := parts[1]
	group := GetGroup(groupName)
	if group == nil {
		http.Error(w, "no such group:"+groupName, http.StatusNotFound)
		return
	}

	view, err := group.Get(key)
	if err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}
	w.Header().Set("Content-Type", "application/octet-stream")
	w.Write(view.ByteSlice())
	w.Write([]byte("\r\n"))
}
~~~

接下来测试：

~~~go
package main

import (
	"fmt"
	"log"
	"net/http"

	v4 "github.com/go-examples-with-tests/net/http/v4"
)

func main() {
	var db = map[string]string{
		"Tom":  "630",
		"Jack": "589",
		"Sam":  "567",
	}

	v4.NewGroup("scores", 2<<10, v4.GetterFunc(func(key string) ([]byte, error) {
		log.Println("[SlowDB] search key ", key)
		if v, ok := db[key]; ok {
			return []byte(v), nil
		}
		return nil, fmt.Errorf("%s not exist", key)
	}))

	addr := ":9999"
	peers := v4.NewHTTPPool(addr)
	log.Println("geecache is running at ", addr)
	log.Fatal(http.ListenAndServe(addr, peers))
}
~~~

上面单机已经启动了 HTTP 服务：

~~~shell
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Sam
567
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Jack
589
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Tom 
630
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Katyusha
Katyusha not exist
~~~

如果多次访问某个学员的分数情况，比如 Jack，由于缓存系统已经存储了，不再去 DB 中查找，直接从缓存中取。

节点间的相互**通信**不仅需要 HTTP 服务端，还需要 HTTP 客户端，这就是我们下一步需要做的事情。

# 4 一致性哈希 Hash

**一致性哈希算法**是缓存系统从**单节点**走向**分布式节点**的一个重要的环节！

**对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据**？假设包括一个有 10 个节点，第一次随机选取了节点1， 节点1从**数据源**获取到数据的同时**缓存该数据**；那第二次只有 1/10 的可能性再次选择节点1，有9/10的概率选择了其他节点，如果选择了其他节点，就意味着需要**再次**从数据源获取数据，一般来说，这个操作是耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

> 整个分布式缓存系统是怎么运作的？**后端服务**如果发现自己没有特定的一个缓存，那该请求哪个节点？

有什么办法对于**给定的 key**，每一次都选择同一个节点？使用 hash 算法，**计算出 key 的哈希值**，再除以整个节点数量取余数，得到的就是节点索引。这样做，相当于是**自定义哈希算法**得到了对应单机缓存系统的位置索引，也就是**找到了当前缓存存储的位置**：

![](./img/hash_select.png)

从上面的图可以看到，任意一个节点任意时刻请求查找键 `Tom` 对应的值，都会分配给节点 2，有效地解决了上述的问题。

简单求取哈希值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设某种场景中移除了一台节点，只剩下 9 个单节点的分布式缓存系统，那么之前的 `hash(key)%10` 变成了 `hash(key)%9`，也就意味着几乎缓存值对应的节点都发生了改变，即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新取数据源获取数据，容易引起**缓存雪崩**——**缓存在同一时刻全部失效**，造成瞬时 DB 请求量大、压力骤增，引起雪崩。常因为缓存服务器宕机，或缓存设置了相同的过期时间引起的。

一致性哈希算法就是为了解决这个问题！一致性哈希算法将 key **映射（这个映射的过程就是计算哈希值的过程）**到 2^32^ 的空间中，将**这个数字**首尾相连，形成一个环：

* 计算节点/机器（通常使用节点的名称、编号和 IP 地址）的**哈希值**，放置在环上；
* 计算 key 的**哈希值**，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

![](./img/add_peer.png)

环上有 peer2，peer4，peer6 三个节点，`key11`，`key2`，`key27` 均映射到 peer2，`key23` 映射到 peer4。此时，如果新增节点/机器 peer8，假设它新增位置如图所示，那么只有 `key27` 从  peer2 调整到  peer8，**其余的映射均没有发生改变**。也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的**一小部分数据**，而不需要重新定位**所有的节点**，这就解决了上述的问题。这一小部分数据仍然需要从数据源获取数据！

如果服务器的节点过少，容易**引起 key 的倾斜**。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，**缓存节点间负载不均**。为了解决这个问题，引入了**虚拟节点**的概念，一个真实节点对应多个虚拟节点。假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是  peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

- 第一步，计算虚拟节点的 Hash 值，放置在环上。
- 第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

一致性哈希算法的实现：

~~~go
package v4

import (
	"hash/crc32"
	"sort"
	"strconv"
)

type Hash func(data []byte) uint32

type Map struct {
	hash     Hash
	replicas int            // 扩充虚拟节点
	keys     []int          // 所有节点根据 hash 值排序（0~2^31-1）
	hashMap  map[int]string // 节点 hash 值和节点名称的映射
}

func New(replicas int, fn Hash) *Map {
	m := &Map{
		replicas: replicas,
		hash:     fn,
		hashMap:  make(map[int]string),
	}
	if m.hash == nil {
		m.hash = crc32.ChecksumIEEE
	}
	return m
}

// 向 Map 中新增节点
func (m *Map) Add(keys ...string) {
	for _, key := range keys {
		// 一个缓存节点扩展成 m.replicas 个节点，相当于增加了虚拟节点
		for i := 0; i < m.replicas; i++ {
			hash := int(m.hash([]byte(strconv.Itoa(i) + key)))
			m.keys = append(m.keys, hash)
			m.hashMap[hash] = key // key 可认为是某个节点，也就是对一个的某个缓存系统
		}
	}
	// 让所有节点按照 hash 的大小依次顺序排列在整个 0 ~ 2^31-1 组成的环上
	sort.Ints(m.keys)
}

func (m *Map) Get(key string) string {
	if len(m.keys) == 0 {
		return "" // 表示没有匹配到任何缓存系统节点
	}
	hash := int(m.hash([]byte(key)))
	idx := sort.Search(len(m.keys), func(i int) bool {
		return m.keys[i] >= hash
	})
	// circle index
	return m.hashMap[m.keys[idx%len(m.keys)]]
}
~~~

测试用例：

~~~go
package v4

import (
	"log"
	"strconv"
	"testing"
)

func TestHash(t *testing.T) {
	hash := New(3, func(data []byte) uint32 {
		// "06" --> 6; "16" --> 16
		i, _ := strconv.Atoi(string(data))
		return uint32(i)
	})

	hash.Add("6", "4", "2")

	testCases := map[string]string{
		"2":  "2",
		"11": "2",
		"23": "4",
		"27": "2",
	}

	for k, v := range testCases {
		got := hash.Get(k)
		log.Printf("ask for %s, got: %s", k, got)
		if got != v {
			t.Fatalf("Asking for %s, should have yielded %s", k, v)
		}
	}

	hash.Add("8")
	testCases["27"] = "8"
	for k, v := range testCases {
		got := hash.Get(k)
		log.Printf("ask for %s, got: %s", k, got)
		if got != v {
			t.Fatalf("Asking for %s, should have yielded %s", k, v)
		}
	}
}
~~~

一致性哈希算法的模型，其功能主要就是用来选择分布式缓存系统中的某个特定节点，指定内容的缓存会存储在某个特定的节点上。在下一节的分布式节点上，`hashMap  map[int]string`  中的 string 部分用于存储节点名，比如：IP 地址、端口号等。

# 5 分布式节点

分布式缓存系统的**主体逻辑**是：

~~~shell
                            是
接收 key --> 检查是否被缓存 -----> 返回缓存值 ⑴
                |  否                         是
                |-----> 是否应当从远程节点获取 -----> 与远程节点交互 --> 返回缓存值 ⑵
                            |  否
                            |-----> 调用`回调函数`，获取值并添加到缓存 --> 返回缓存值 ⑶
~~~

其中步骤 2 还可以**进一步细化**：

~~~shell
使用一致性哈希选择节点        是                                    是
    |-----> 是否是远程节点 -----> HTTP 客户端访问远程节点 --> 成功？-----> 服务端返回返回值
                    |  否                                    ↓  否
                    |----------------------------> 回退到本地节点处理。
~~~

到目前为止分布式缓存系统还没有实现的逻辑：

1. 步骤 2 中使用一致性哈希选择节点的抽象；
2. 远程节点的抽象；
3. 通过 HTTP 请求访问远程节点的抽象。

抽象出接口：

~~~go
package v4

type PeerPicker interface {
	PickPeer(key string) (PeerGetter, bool)
}

type PeerGetter interface {
	Get(group string, key string) ([]byte, error)
}
~~~

PeerGetter 表示远程节点需要具备的能力，抽象出远程节点数据结构，将这种映射关系保存到本地；PeerPicker 则是封装了挑选远端节点的能力，表示能够从众多的远端节点中依据一致性哈希选出待访问的节点。

由 PeerGetter 接口，扩展出 httpGetter 结构体，表示一个远端节点：

~~~go
package v4

import (
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
	"net/url"
)

// httpGetter send HTTP request to specific Server, and get the cache value
type httpGetter struct {
	baseURL string
}

func (getter *httpGetter) Get(group string, key string) ([]byte, error) {
	u := fmt.Sprintf("%v%v/%v",
		getter.baseURL, url.QueryEscape(group), url.QueryEscape(key))

	// http://localhost:8003/_geecache/scores/Katyusha
	log.Printf("httpGetter send request to: %v", u)
	// 依据指定的 URL 发出请求，等待 Server 响应并回传 cache value
	response, err := http.Get(u)
	if err != nil {
		return nil, err
	}
	defer response.Body.Close()

	if response.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("server returned: %v", response.Status)
	}

	res, err := ioutil.ReadAll(response.Body)
	if err != nil {
		return nil, fmt.Errorf("reading body error:%v", err)
	}
	return res, nil
}
~~~

由 PeerPicker 接口，**为 HTTPPool 结构体赋能**：

~~~go
package v4

import (
	"fmt"
	"log"
	"net/http"
	"strings"
	"sync"

	"github.com/go-examples-with-tests/net/http/v4/consistenthash"
)

const (
	defaultBasePath = "/_geecache/"
	defaultReplicas = 50
)

// 承载节点 HTTP 通信的核心数据结构，是分布式缓存系统的一个节点
// 作为通信的服务端，也就是接收客户端的 HTTP 缓存请求
type HTTPPool struct {
	self     string
	basePath string // API前缀，默认是 defaultBasePath

	mu          sync.Mutex
	peers       *consistenthash.Map    // hash(key) 选择 http://10.0.0.2:8008
	httpGetters map[string]*httpGetter // http://10.0.0.2:8008 --> *httpGetter
}

func (p *HTTPPool) Set(peers ...string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	// 在本地为远端分布式节点建立模型，也就是将自身看作是 Client，从远端节点获取 cache value
	p.peers = consistenthash.New(defaultReplicas, nil)
	// add all peers
	log.Printf("HTTPPool add peers: %v", peers)
	// http://localhost:8001, http://localhost:8002, http://localhost:8003...
	p.peers.Add(peers...)
	p.httpGetters = make(map[string]*httpGetter)

	for _, peer := range peers {
		p.httpGetters[peer] = &httpGetter{
			baseURL: peer + p.basePath,
		}
	}
}

func (p *HTTPPool) PickPeer(key string) (PeerGetter, bool) {
	p.mu.Lock()
	defer p.mu.Unlock()

	peer := p.peers.Get(key)
	log.Printf("get peer in peers: %v, p.self: %v", peer, p.self)
	if peer != "" && peer != p.self {
		// peer != p.self 时，发起远端 HTTP 请求；如果是，则直接在本地请求
		return p.httpGetters[peer], true
	}
	return nil, false
}
~~~

分布式缓存系统的主体流程变更为：

~~~go
package v4

import (
	"fmt"
	"log"
	"sync"
)

type Group struct {
	name      string
	getter    Getter
	mainCache cache //FIXME 此处为什么不能是 *cache？什么时候使用指针，什么时候使用普通类型？

	picker PeerPicker
}

var (
	mu sync.RWMutex

	// 每一个进程中有多个命名不同的 Group
	groups = make(map[string]*Group) //FIXME 此处为什么存的是 *Group？
)

type Getter interface {
	Get(key string) ([]byte, error)
}

type GetterFunc func(key string) ([]byte, error) // 接口型函数，即函数实现Getter接口

func (f GetterFunc) Get(key string) ([]byte, error) {
	return f(key) // 实质上，函数的声明和接口中定义的功能声明相同；实现接口方法时，调用了自身
}

...

func (g *Group) RegistePeers(picker PeerPicker) {
	if g.picker != nil {
		panic("RegistePeers called more than once")
	}
	g.picker = picker
}

func (g *Group) Get(key string) (ByteView, error) {
	if key == "" {
		return ByteView{}, fmt.Errorf("key is required")
	}
	if v, ok := g.mainCache.get(key); ok {
		log.Println("[GeeCache] hit")
		return v, nil
	}

	return g.load(key)
}

func (g *Group) load(key string) (value ByteView, err error) {
	if g.picker != nil {
		if peer, ok := g.picker.PickPeer(key); ok {
			if value, err = g.getFromPeer(peer, key); err == nil {
				return value, nil
			}
			log.Println("[GeeCache] Failed to get from peer ", err)
		}
	}
	// 调用 getter，用户自定义获取数据方式
	return g.getLocally(key)
}

func (g *Group) getFromPeer(peer PeerGetter, key string) (ByteView, error) {
	cache, err := peer.Get(g.name, key)
	if err != nil {
		return ByteView{}, err
	}
	return ByteView{b: cloneBytes(cache)}, nil
}
...
~~~

分布式缓存系统的主体流程分为 3 个步骤：

1. `func (g *Group) Get(key string) (ByteView, error) `
2. `func (g *Group) load(key string) (value ByteView, err error)`
3. `func (g *Group) getFromPeer(peer PeerGetter, key string) (ByteView, error)`

其中有一个关键过程：

~~~go
func (p *HTTPPool) PickPeer(key string) (PeerGetter, bool) {
	p.mu.Lock()
	defer p.mu.Unlock()

	peer := p.peers.Get(key)
	log.Printf("get peer in peers: %v, p.self: %v", peer, p.self)
	if peer != "" && peer != p.self {
		// peer != p.self 时，发起远端 HTTP 请求；如果是，则直接在本地请求
		return p.httpGetters[peer], true
	}
	return nil, false
}
~~~

主站在选择远端节点时，需要判断根据一致性哈希选择的远端节点是否是 `p.self`——“自己”，如果自己，则直接返回并在本地获取数据。反之，则通过发起 HTTP 请求。

测试程序：

~~~go
package main

import (
	"flag"
	"fmt"
	"log"
	"net/http"

	v4 "github.com/go-examples-with-tests/net/http/v4"
)

var db = map[string]string{
	"Tom":  "630",
	"Jack": "589",
	"Sam":  "567",
}

func main() {
	var port int
	var api bool
	flag.IntVar(&port, "port", 8001, "Geecache server port")
	flag.BoolVar(&api, "api", false, "Start a api server?") // 什么含义？
	flag.Parse()

	log.Printf("api:%v", api)

	apiAddr := "http://localhost:9999"
	peerMap := map[int]string{
		8001: "http://localhost:8001",
		8002: "http://localhost:8002",
		8003: "http://localhost:8003",
	}

	var addrs []string
	for _, v := range peerMap {
		addrs = append(addrs, v)
	}

	gee := createGroup()
	if api {
		go startAPIServer(apiAddr, gee)
	}
	// 默认选择 8001 端口作为主站启动，并可接收
	startCacheServer(peerMap[port], addrs, gee)
}

func createGroup() *v4.Group {
	return v4.NewGroup("scores", 2<<10, v4.GetterFunc(func(key string) ([]byte, error) {
		// 模拟从本地DB中取值
		log.Println("[SlowDB] search key", key)
		if v, ok := db[key]; ok {
			return []byte(v), nil
		}
		return nil, fmt.Errorf("%s not exist", key)
	}))
}

func startCacheServer(addr string, addrs []string, gee *v4.Group) {
	httppool := v4.NewHTTPPool(addr)
	httppool.Set(addrs...)
	gee.RegistePeers(httppool)

	log.Println("geecache is running at", addr)

	log.Fatal(http.ListenAndServe(addr[7:], httppool))
}

func startAPIServer(apiAddr string, gee *v4.Group) {
	http.Handle("/api", http.HandlerFunc(func(rw http.ResponseWriter, r *http.Request) {
		key := r.URL.Query().Get("key")
		view, err := gee.Get(key)
		if err != nil {
			http.Error(rw, err.Error(), http.StatusInternalServerError)
			return
		}
		rw.Header().Set("Content-Type", "application/octet-stream")
		rw.Write(view.ByteSlice())
		rw.Write([]byte("\r\n"))
	}))
	log.Println("fontend server is running at", apiAddr)
	log.Fatal(http.ListenAndServe(apiAddr[7:], nil))
}
~~~

在分布式缓存系统中，有一个比较难理解的地方：

“哪个站点会接收缓存请求，如果当前站点没有缓存，是会去请求其他站点吗？”

再一次看**主流程图**：

~~~shell
                            是
接收 key --> 检查是否被缓存 -----> 返回缓存值 ⑴
                |  否                         是
                |-----> 是否应当从远程节点获取 -----> 与远程节点交互 --> 返回缓存值 ⑵
                            |  否
                            |-----> 调用`回调函数`，获取值并添加到缓存 --> 返回缓存值 ⑶
~~~

细化的第2个步骤：

~~~shell
使用一致性哈希选择节点        是                                    是
    |-----> 是否是远程节点 -----> HTTP 客户端访问远程节点 --> 成功？-----> 服务端返回返回值
                    |  否                                    ↓  否
                    |----------------------------> 回退到本地节点处理。
~~~

系统架构背后的一个关键点是：分布式缓存系统的所有节点使用的是**相同的一致性哈希算法**，也就是不管是哪个节点在选择对端节点时，一致性哈希算法都是相同的。

由此测试脚本可以这样写：

~~~bash
#!/bin/bash
trap "rm server;kill 0" EXIT

go build -o server
./server -port=8001 &
./server -port=8002 &
./server -port=8003 -api=1 &

sleep 2
echo ">>> start test"
curl "http://localhost:9999/api?key=Tom" &
curl "http://localhost:9999/api?key=Tom" &
curl "http://localhost:9999/api?key=Tom" &

wait
~~~

脚本中启动了 3 个进程，每个进程对应接收 HTTP 请求的端口号是不同的，也就是 3 个不同分布式缓存系统的节点。

此处，引出一个问题：并发发起 3 个不同的缓存请求，`curl "http://localhost:9999/api?key=Tom"` 会同时选择 `8001` 这个端口对应的进程。也就是说，`8003` 端口这个进程会向 `8001` 端口这个进程发起 3 个 HTTP 请求。试想，假如有 10 万个在并发请求该数据呢？那就会向 `8001` 同时发起 10 万次请求，如果 `8001` 又同时向数据库发起 10 万次查询请求，很容易导致**缓存被击穿**。三次请求的结果是一致的，**对于相同的 key，能不能只向 `8001` 发起一次请求**？这个问题下一次解决。

# 6 防止缓存击穿

理清概念：在真实的业务场景中，考虑缓存问题，需要考虑：缓存时效时间和 key 是否存在

* **缓存雪崩**：缓存在**同一时刻全部失效**，造成瞬时DB请求量大、压力骤增，引起雪崩。缓存雪崩通常因为缓存服务器宕机、缓存的 key 设置了相同的过期时间等引起。

  **解决方案**：在原有设置缓存失效时间上添加一个随机时间，比如 1~5 分钟，这样避免了采用相同的过期时间导致的缓存雪崩；（**兜底机制**）如果真的发生了缓存雪崩，当流量达到一定阈值时，直接返回“系统拥挤”，至少能够保证一部分用户的正常访问；另外，还可以提高数据库的容灾能力，使用分库分表、读写分离策略；可以搭建 Redis 集群，提高 Redis 的容灾能力。

* **缓存击穿**：一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到 DB ，造成瞬时DB请求量大、压力骤增。特点在与：**某个特定的、热点的 key**，在缓存**失效**后，有**大并发的请求**集中对其访问，导致大并发全部打在数据库上。

  **解决方案**：如果业务允许，可以对特定的、热点的 key 不设置时效时间；使用**互斥锁**，只有拿到锁的请求，才能访问数据库，降低同一时间打在数据库上的请求，防止数据库被打死，但这样做会导致**性能下降**。

* **缓存穿透**：查询一个不存在的数据（在存储系统中，或者在所有的业务场景下），因为不存在则不会写到缓存中，所以每次都会去请求 DB，如果瞬间流量过大，穿透到 DB，导致宕机。关键在于在 Redis（分布式缓存系统）查不到 key 值，这和缓存击穿有根本的区别，区别在于**缓存穿透的情况是传进来的key在Redis中是不存在的**。假如有黑客传进大量的不存在的key，那么大量的请求打在数据库上是很致命的问题，所以在日常开发中要**对参数做好校验**，一些非法的参数，不可能存在的key就直接返回错误提示，要对调用方保持这种“不信任”的心态。

  **解决方案**：在查询 Redis 之前，增加一层布隆过滤器的查询。**布隆过滤器**的作用是某个 key 不存在，那么就一定不存在，它说某个 key 存在，那么很大可能是存在(存在一定的**误判率**)。于是我们可以在缓存之前再加一层布隆过滤器，在查询的时候先去布隆过滤器查询 key 是否存在，如果不存在就直接返回。

在使用Redis的时候是肯定会遇到的，而且是**非常致命性**的问题。还有一个需要注意的是要**做好熔断**，一旦出现缓存雪崩，击穿，穿透这种情况，至少还有熔断机制**保护数据库不会被打死**。

接下来的部分就是针对**缓存击穿**场景下使用互斥锁解决方案的实现：

~~~go
package singleflight

import "sync"

type Group struct {
	mu sync.Mutex
	m  map[string]*call
}

type call struct {
	wg sync.WaitGroup

	// 用于保存相同 key，通过 HTTP 获取到的缓存结果
	val interface{}
	err error
}

func (g *Group) Do(key string, fun func() (interface{}, error)) (interface{}, error) {
	g.mu.Lock()
	// 延迟加载，懒加载
	if g.m == nil {
		g.m = make(map[string]*call)
	}

	if c, ok := g.m[key]; ok {
		g.mu.Unlock() // 此处必须加上解锁！
		c.wg.Wait()   // 如果没有任何 goroutine 执行了 c.wg.Add(1)，会有什么影响？
		return c.val, c.err
	}

	c := new(call) // *call 类型
	c.wg.Add(1)
	g.m[key] = c
	g.mu.Unlock()

	c.val, c.err = fun()
	c.wg.Done()

	g.mu.Lock()
	delete(g.m, key)
	g.mu.Unlock()

	return c.val, c.err
}
~~~

数据结构 call 代表正在进行中，或已经结束的请求。使用 `sync.WaitGroup` 锁是为了避免多个 goroutine 重入（即是 Group 的命名含义）。Group 数据结构是 singleflight 的主要数据结构，管理不同 key 的请求。其中 g.mu 是保护成员变量 g.m 不被并发读写（map 是不支持并发读写的）。

Do 方法，接收 2 个参数，第一个参数是 `key`，第二个参数是一个函数 `fn`。Do 的作用就是，针对相同的 key，无论 Do 被调用多少次，函数 `fn` 都只会被调用一次，等待 fn 调用结束了，返回返回值或错误。

**并发协程之间不需要消息传递**，非常适合 `sync.WaitGroup`。

- wg.Add(1) 锁加1。
- wg.Wait() 阻塞，直到锁被释放。
- wg.Done() 锁减1。

singleflight 的使用：

~~~go
type Group struct {
	name      string
	getter    Getter
	mainCache cache //FIXME 此处为什么不能是 *cache？什么时候使用指针，什么时候使用普通类型？

	picker       PeerPicker
	singleflight *singleflight.Group
}

...

func NewGroup(name string, cacheBytes int64, getter Getter) *Group {
	if getter == nil {
		panic("getter is nil")
	}

	if _, exist := groups[name]; exist {
		panic("group " + name + " exists")
	}

	mu.Lock()
	defer mu.Unlock()
	g := &Group{
		name:         name,
		getter:       getter,
		mainCache:    cache{cacheBytes: cacheBytes},
		singleflight: &singleflight.Group{},
	}
	groups[name] = g
	return g
}

...

func (g *Group) load(key string) (value ByteView, err error) {
	// 不论是从远端获取还是本地获取，都仅做一次请求
	view, err := g.singleflight.Do(key, func() (interface{}, error) {
		if g.picker != nil {
			if peer, ok := g.picker.PickPeer(key); ok {
				if value, err = g.getFromPeer(peer, key); err == nil {
					return value, nil
				}
				log.Println("[GeeCache] Failed to get from peer ", err)
			}
		}
		// 调用 getter，用户自定义获取数据方式
		return g.getLocally(key)
	})

	if err == nil {
		return view.(ByteView), nil
	}
	return
}
~~~

不论是本地获取数据，还是通过远端的节点获取数据，在面对多个获取缓存请求时，都只做一次！

# 7 使用 Protobuf 通信

protobuf 即 Protocol Buffers，Google 开发的**一种数据描述语言**，是一种轻便高效的**结构化数据存储格式**，与语言、平台无关，可扩展可序列化。

protobuf 广泛地应用于远程过程调用 RPC 的二进制传输。使用 protobuf 的目的非常简单，为了**获得更高的性能**。传输前使用 protobuf **编码**，接收方再（按照相同的格式 protobuf）进行**解码**，可以显著地降低二进制传输的大小。另一方面，protobuf 非常适合传输结构化数据，便于通信字段的扩展。

> 如果不进行解码，直接保存到本地，是否可用于数据保存？

使用 protobuf 一般分为以下 2 步：

- 按照 protobuf 的语法，在 `.proto` 文件中定义数据结构，并使用 `protoc` 生成 Go 代码（`.proto` 文件是跨平台的，还可以生成 C、Java 等其他源码文件）。
- 在项目代码中引用生成的 Go 代码。

在分布式缓存的例子中，通信数据是：

~~~protobuf
syntax = "proto3";

package cachepb;

option go_package="../cachepb"; // 指定 cachepb.pb.go 文件生成的目录路径

message Request {
    string group = 1;
    string key = 2;
}

message Response {
    bytes value = 1;
}

service GroupCache {
    rpc Get(Request) returns (Response);
}
~~~

对应生成的 `.go` 文件的指令：

~~~shell
ant@MacBook-Pro cachepb % protoc --go_out=. *.proto
protoc-gen-go: unable to determine Go import path for "cachepb.proto"

Please specify either:
        • a "go_package" option in the .proto source file, or
        • a "M" argument on the command line.

See https://developers.google.com/protocol-buffers/docs/reference/go-generated#package for more information.

--go_out: protoc-gen-go: Plugin failed with status code 1.
~~~

需要增加 `option go_package="../cachepb"; // 指定 cachepb.pb.go 文件生成的目录路径`，否则报错！

对应生成的 `.go` 文件中，有对应的数据结构：

~~~go
type Request struct {
	state         protoimpl.MessageState
	sizeCache     protoimpl.SizeCache
	unknownFields protoimpl.UnknownFields

	Group string `protobuf:"bytes,1,opt,name=group,proto3" json:"group,omitempty"`
	Key   string `protobuf:"bytes,2,opt,name=key,proto3" json:"key,omitempty"`
}

type Response struct {
	state         protoimpl.MessageState
	sizeCache     protoimpl.SizeCache
	unknownFields protoimpl.UnknownFields

	Value []byte `protobuf:"bytes,1,opt,name=value,proto3" json:"value,omitempty"`
}
~~~

也就是将我们原先自定义的数据结构做 protobuf 格式转换，变成了 protobuf 能够认识的结构体。

紧接着，修改所有 Server-Client 交互的接口部分，修改为全部使用 protobuf 格式：

~~~go
type PeerGetter interface {
	// Get(group string, key string) ([]byte, error)

	Get(in *cachepb.Request, out *cachepb.Response) error // 替换成使用 protobuf，作为通信信息的格式
}
...
~~~

指定 Peer 端通过 HTTP 协议获取对端数据时，使用的是 protobuf 格式。如下，就是 Client 端发出 HTTP 请求后，接收到 Response 时，按照 protobuf 格式解析数据：

~~~go
func (getter *httpGetter) Get(in *cachepb.Request, out *cachepb.Response) error {
	u := fmt.Sprintf("%v%v/%v",
		getter.baseURL, url.QueryEscape(in.Group), url.QueryEscape(in.Key))

	// http://localhost:8003/_geecache/scores/Katyusha
	log.Printf("httpGetter send request to: %v", u)
	// 依据指定的 URL 发出请求，等待 Server 响应并回传 cache value
	response, err := http.Get(u)
	if err != nil {
		log.Println("client Get: " + err.Error())
		return err
	}
	defer response.Body.Close()

	if response.StatusCode != http.StatusOK {
		return fmt.Errorf("server returned: %v", response.Status)
	}

	bytes, err := ioutil.ReadAll(response.Body)
	if err != nil {
		return fmt.Errorf("reading body error:%v", err)
	}

	// 通信数据的格式是 protobuf，解码
	if err = proto.Unmarshal(bytes, out); err != nil {
        // proto: cannot parse invalid wire-format data
		return fmt.Errorf("decoding response body: %v", err)
	}
	return nil
}
~~~

与之对应的，Server 端接收到 Request 后，反馈 Response：

~~~go
// ServeHTTP serve client HTTP request, and response the cache value
func (p *HTTPPool) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	if !strings.HasPrefix(r.URL.Path, p.basePath) {
		panic("HTTPPool serving unexpected path: " + r.URL.Path)
	}
	p.Log("%s %s", r.Method, r.URL.Path)

	// /<basepath>/<groupname>/<key>
	parts := strings.SplitN(r.URL.Path[len(p.basePath):], "/", 2)
	if len(parts) != 2 {
		http.Error(w, "bad request", http.StatusBadRequest)
		return
	}

	groupName := parts[0]
	key := parts[1]
	group := GetGroup(groupName)
	if group == nil {
		http.Error(w, "no such group:"+groupName, http.StatusNotFound)
		return
	}

	// get cache value
	view, err := group.Get(key)
	if err != nil {
		log.Println("group.Get(key):", err.Error())
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}

	// 通信数据的格式是 protobuf，编码
	body, err := proto.Marshal(&cachepb.Response{Value: view.ByteSlice()})
	if err != nil {
		log.Println("proto.Marshal:", err.Error())
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/octet-stream")
	w.Write(body)
	// w.Write([]byte("\r\n"))
}
~~~

其中，关键的一步就是将结果以 protobuf 的格式编码，并写入到 ResponseWriter 中。

与之对应的测试程序：

~~~go
package main

import (
	"flag"
	"fmt"
	"log"
	"net/http"

	v4 "github.com/go-examples-with-tests/net/http/v4"
)

var db = map[string]string{
	"Tom":  "630",
	"Jack": "589",
	"Sam":  "567",
}

func main() {
	var port int
	var api bool
	flag.IntVar(&port, "port", 8001, "Geecache server port")
	flag.BoolVar(&api, "api", false, "Start a api server?") // 什么含义？
	flag.Parse()

	log.Printf("api:%v", api)

	apiAddr := "http://localhost:9999"
	peerMap := map[int]string{
		8001: "http://localhost:8001",
		8002: "http://localhost:8002",
		8003: "http://localhost:8003",
	}

	var addrs []string
	for _, v := range peerMap {
		addrs = append(addrs, v)
	}

	gee := createGroup()
	if api {
		go startAPIServer(apiAddr, gee)
	}
	// 默认选择 8001 端口作为主站启动，并可接收
	startCacheServer(peerMap[port], addrs, gee)
}

func createGroup() *v4.Group {
	return v4.NewGroup("scores", 2<<10, v4.GetterFunc(func(key string) ([]byte, error) {
		// 模拟从本地DB中取值
		log.Println("[SlowDB] search key", key)
		if v, ok := db[key]; ok {
			return []byte(v), nil
		}
		return nil, fmt.Errorf("%s not exist", key)
	}))
}

func startCacheServer(addr string, addrs []string, gee *v4.Group) {
	httppool := v4.NewHTTPPool(addr)
	httppool.Set(addrs...)
	gee.RegistePeers(httppool)

	log.Println("geecache is running at", addr)

	log.Fatal(http.ListenAndServe(addr[7:], httppool))
}

func startAPIServer(apiAddr string, gee *v4.Group) {
	http.Handle("/api", http.HandlerFunc(func(rw http.ResponseWriter, r *http.Request) {
		key := r.URL.Query().Get("key")
		view, err := gee.Get(key)
		if err != nil {
			http.Error(rw, err.Error(), http.StatusInternalServerError)
			return
		}
		rw.Header().Set("Content-Type", "application/octet-stream")
		rw.Write(view.ByteSlice())
		rw.Write([]byte("\r\n"))
	}))
	log.Println("fontend server is running at", apiAddr)
	log.Fatal(http.ListenAndServe(apiAddr[7:], nil))
}
~~~

测试脚本：

~~~bash
#!/bin/bash
trap "rm server;kill 0" EXIT

go build -o server
./server -port=8001 &
./server -port=8002 &
./server -port=8003 -api &

wait
~~~

脚本的含义：

1. 创建 3 个进程，并且运行的是相同的程序；
2. 3 个不同的进程，不同之处是 8003 端口所在的进程额外监听了 9999 端口，能通过 `curl "http://localhost:9999/api?key=Tom"` 查询缓存值；
3. 这 3 个不同的进程，是分布式缓存系统中的不同节点，但各个节点使用的一致性哈希算法是相同的。也就是说，不管请求哪个进程的端口，如果本地没有缓存，通过 key 计算得到的目标节点肯定是一致的。
4. 如果请求的目标进程没有这个缓存，那么会通过 HTTP 请求，此时本地进程相当于是 Client，一致性哈希算法得到的节点就是 Server。整个通信过程中数据格式是 protobuf。

运行测试脚本得到的日志：

~~~shell
ant@MacBook-Pro http % ./run.sh
2021/10/06 17:47:57 api:false
2021/10/06 17:47:57 api:false
2021/10/06 17:47:57 api:true
2021/10/06 17:47:57 HTTPPool add peers: [http://localhost:8001 http://localhost:8002 http://localhost:8003]
2021/10/06 17:47:57 HTTPPool add peers: [http://localhost:8002 http://localhost:8003 http://localhost:8001]
2021/10/06 17:47:57 HTTPPool add peers: [http://localhost:8001 http://localhost:8002 http://localhost:8003]
2021/10/06 17:47:57 fontend server is running at http://localhost:9999
2021/10/06 17:47:57 geecache is running at http://localhost:8001
2021/10/06 17:47:57 geecache is running at http://localhost:8002
2021/10/06 17:47:57 geecache is running at http://localhost:8003
2021/10/06 17:48:00 get peer in peers: http://localhost:8001, p.self: http://localhost:8003
2021/10/06 17:48:00 g.name:scores, key:Tom
2021/10/06 17:48:00 httpGetter send request to: http://localhost:8001/_geecache/scores/Tom
2021/10/06 17:48:00 [Server http://localhost:8001] GET /_geecache/scores/Tom
2021/10/06 17:48:00 get peer in peers: http://localhost:8001, p.self: http://localhost:8001
2021/10/06 17:48:00 [SlowDB] search key Tom
2021/10/06 17:48:04 get peer in peers: http://localhost:8001, p.self: http://localhost:8003
2021/10/06 17:48:04 g.name:scores, key:Tom
2021/10/06 17:48:04 httpGetter send request to: http://localhost:8001/_geecache/scores/Tom
2021/10/06 17:48:04 [Server http://localhost:8001] GET /_geecache/scores/Tom
2021/10/06 17:48:04 [GeeCache] hit
~~~

另外关于 curl 的经验：

1. `curl "http://localhost:9999/api?key=Tom"` 必须使用英文双引号将 url 扩起来，否则 Server 端会认为 url 是错误的；
2. 启动上述脚本后，执行 `curl http://localhost:8001/_geecache/scores/Tom` 时，返回的 protobuf 格式数据能够被 curl 解析。

# 总结

1. 在后端缓存应用中，内存资源是有限的。为了解决缓存淘汰的问题，引出了 3 种方案并分别描述了缓存淘汰策略的特点：FIFO、LFU、LRU，前两种淘汰算法仅考虑的是缓存被添加的时间和缓存使用的频率，最后一种特征比较平衡。LRU 实现的关键数据结构：hashmap 和双端队列。
2. 应用对缓存系统的访问并不是串行的，但 map 是不支持并发读和并发写的，由此使用 sync.Mutex 实现了单机版的缓存系统，支持并发读写。
3. 为了将应用从单机部署扩展到多机，也就是支持分布式集群，为缓存应用增加 HTTP 通信功能，实现节点间通信。如果一个节点启动了 HTTP 服务，那么这个节点就可以被其他节点访问。
4. 节点间的通信，不仅需要 HTTP 服务端，还需要 HTTP 客户端。一致性哈希算法是缓存系统从单节点走向分布式节点的重要环节。对于分布式缓存来说，面临这样的问题：当一个节点收到请求后，如果该节点并没有存储缓存值，那么它会从谁那里获取数据？而且还要解决缓存雪崩、key 倾斜的问题。
5. 分布式缓存系统需要具备通信功能，那么就需要客户端功能，这个和 Server 端是对应的。Client 的功能，相当于是向 Server 端发起 HTTP 请求，获取 Server 端查询缓存的反馈。
6. 应用互斥锁解决缓存击穿的问题。
7. 应用 protobuf 格式做 HTTP 通信的数据格式，提升通信效率。