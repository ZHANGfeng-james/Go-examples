第一次请求时将一些**耗时操作的结果**暂存（存），以后遇到**相同的请求**（比较），（取）直接**返回暂存的数据**。整个过程涉及到 3 个方面动作：存、比较和取。

在计算机系统中，缓存无处不在。比如我们访问一个网页，网页和引用的 JS/CSS 等静态文件，根据不同的策略，会缓存在**浏览器本地**或是 **CDN 服务器**，那在第二次访问的时候，就会觉得网页加载的速度快了不少。再比如，微博的点赞数量，不可能每个客户端的每次访问，都从数据库中查找所有点赞的记录再做统计，**数据库的操作是很耗时的**，难以支持这么大的流量。所以，一般点赞这类数据是缓存在 Redis 服务器集群中的。

> 浏览器，是属于本地运行的服务；CDN 服务器则是对应服务端的设备服务。

商业世界里，现金为王；架构世界里，缓存为王！

**缓存**中最简单的莫过于**存储在内存中**的键值对缓存了。键值对，在 Go 中称之为 map。那直接创建一个 map，每次有新数据就往 map 中插入，这样做有什么问题？

1. **内存不够了怎么办？**

   向 map 中插入数据，特别是在考虑 map 底层数据结构时，插入数据多了后由于 Hash 碰撞，导致性能急剧下降。那就删除几条数据，是随机删除呢？还是按照时间顺序删除？有没有其他更好的缓冲淘汰策略呢？不同**数据访问频率**是不一样的，优先删除访问频率低的数据是不是更好？**数据的访问频率**可能随着时间变化，那优先删除**最近最少访问**的数据可能是一个更好的选择。

2. **并发写入冲突了怎么办？**

   **对缓存的访问，一般不可能是串行的**。map 是没有并发保护的，应对并发的场景需要加锁。

3. **单机性能不够怎么办？**

   单台计算机的资源是有限的，**计算、存储（指内存）**等都是有限的。

   随着业务量和访问量的增加，单台机器很容易遇到瓶颈。如果**利用多台计算机的资源**，**并行处理**提高性能就要缓存应用能够**支持分布式**，这称为**水平扩展**。与水平扩展相对应的是**垂直扩展**，即通过**增加单个节点的计算、存储、带宽**等，来提高系统的性能，硬件的成本和性能并非呈线性关系，大部分情况下，分布式系统是一个更优的选择。

4. 等等

设计一个**分布式缓存系统**，需要考虑资源控制、淘汰策略、并发、分布式节点通信等各个方面的问题。而且，**针对不同的应用场景，还需要在不同的特性之间权衡**，例如，是否需要支持缓存更新？还是假定缓存在淘汰之前是不允许改变的。不同的权衡对应着不同的实现。接下来的内容是**模仿 [groupcache](https://github.com/golang/groupcache) 的实现**，裁剪了部分功能。但总体实现上，还是与 groupcache 非常接近的。支持特性有：

- 单机缓存和基于 HTTP 的分布式缓存
- 最近最少访问(Least Recently Used, LRU) 缓存策略
- 使用 Go 锁机制防止缓存击穿
- 使用一致性哈希选择节点，实现负载均衡
- 使用 protobuf 优化节点间二进制通信
- ...

接下来，我们就开始设计和实现一个**分布式缓存系统**！

# 1 LRU 缓存淘汰策略

分布式缓存系统中的**所有缓存都存储在内存中**（如果不是存在内存中，那就不能称之为缓存），内存是有限的，由此不可能无限制地添加数据。

假定我们设置缓存能够使用的内存大小为 N，那么在添加了某一条缓存记录之后，占用内存超过了 N（阈值），这个时候就需要从缓存中移除一条或多条数据了。那问题在于移除谁呢？我们肯定希望尽可能移除“没用”的数据，那**如何判定数据“有用”或“没用”呢**？

> 也就是，使用量化关系，将每个数据的特征进行量化，通过比较量化后的值，达到判定数据“有用”的目的。

* **First In First Out**

  意为：**先进先出**，也就是淘汰最早添加的记录。FIFO 认为，最早添加的记录，其不再被使用的可能性比刚添加的可能性大。这种算法的实现非常简单：创建一个队列，新增记录（**判断何时需要新增记录，比如先在缓存中查找，但未找到**）添加到队尾，每次内存不够时，淘汰队首（仅考虑的是记录被添加进来的时间）。但是很多场景下，部分记录虽然是最早添加但也最常被访问，而不得不因为待的时间太长而**被淘汰**。这类数据会被频繁地添加进缓存，又被淘汰出去，导致**缓存命中率**降低。

* **Least Frequently Used**

  最少使用，也就是淘汰缓存中给**访问频率最低的记录**。**LFU 认为，如果数据过去被访问多次，那么将来被访问的频率也更高**。LFU 的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排列，淘汰时选择访问次数最少的即可。LFU 算法的命中率是比较高的，但缺点也非常明显，**维护每个记录的访问次数**，对内存的消耗是很高的；另外，如果数据的**访问模式**发生变化，LFU 需要较长的时间去适应，也就是说 LFU 算法受历史数据的影响比较大。例如某个数据历史上访问次数**奇高**，但在某个时间点之后几乎不再被访问，但应为历史访问次数过高，而迟迟不能被淘汰。

* **Least Recently Used**

  最近最少使用，相对于仅考虑时间因素的 FIFO 和仅考虑访问频率的 LFU，LRU 算法可以认为是**相对平衡的一种淘汰算法**。**LRU 认为，如果数据最近被访问过，那么将来被访问的概率也会更高**。

  LRU 算法的实现非常简单，维护一个队列，如果某条记录被访问了，则移动到队尾。那么队首则是最近最少访问的数据，淘汰该条记录即可。

我要去实现的是这样的数据结构：

![](./img/doubly_linked_list.png)

这张图很好地表示了 LRU 算法**最核心的 2 个数据结构**：

- 绿色的是字典(map)，存储键和值的映射关系。这样根据某个键(key)查找对应的值(value)的复杂是`O(1)`，在字典中插入一条记录的复杂度也是`O(1)`。
- 红色的是双向链表(double linked list)实现的队列。将所有的值放到双向链表中，这样，当访问到某个值时，将其移动到队尾的复杂度是`O(1)`，在队尾新增一条记录以及删除一条记录的复杂度均为`O(1)`。

首先要构造数据结构：

~~~go
type Cache struct {
	maxBytes  int64                         // Cache最大能容纳字节数
	nbytes    int64                         // 当前已装载容量
	ll        *list.List                    // Cache数据结构中双端链表
	cache     map[string]*list.Element      // key-value map，用于找到指定key对应的 *list.Element
	onRemoved func(key string, value Value) // Callback 机制
}
~~~

其中 `Cache.cache` 是为了依据 key 在 `Cache.ll` 中找到对应的节点。

接下来实现 Cache 的功能：

~~~go
type entry struct {
	key   string
	value Value
}

type Value interface {
	Len() int
}

func New(maxBytes int64, onRemoved func(key string, value Value)) *Cache {
	return &Cache{
		maxBytes:  maxBytes,
		ll:        list.New(),
		cache:     make(map[string]*list.Element),
		onRemoved: onRemoved,
	}
}

func (c *Cache) Add(key string, value Value) {
	if ele, ok := c.cache[key]; ok { // ele 类型：*list.Element
		// update the exists Element
		c.ll.MoveToFront(ele)
		kv := ele.Value.(*entry) // ele.Value 类型：*entry
		c.nbytes += int64(value.Len()) - int64(kv.value.Len())
		kv.value = value
	} else {
		// add a new Element
		newNode := c.ll.PushFront(&entry{key, value})
		c.cache[key] = newNode
		c.nbytes += int64(len(key) + value.Len())
	}
	for c.maxBytes != 0 && c.nbytes > c.maxBytes {
		c.RemoveOldest()
	}
}

func (c *Cache) Get(key string) (value Value, ok bool) {
	if ele, ok := c.cache[key]; ok {
		// Cache最前端是最近一次访问的节点，末尾是最早访问的节点
		c.ll.MoveToFront(ele)
		kv := ele.Value.(*entry) // ele.Value 类型：*entry
		return kv.value, true    // 如果直接写成 return 返回的 ok 值为 false
	}
	return
}

func (c *Cache) RemoveOldest() {
	ele := c.ll.Back()
	if ele != nil {
		kv := ele.Value.(*entry)
		delete(c.cache, kv.key)
		c.ll.Remove(ele)
		c.nbytes -= (int64(len(kv.key)) + int64(kv.value.Len()))
		if c.onRemoved != nil {
			c.onRemoved(kv.key, kv.value)
		}
	}
}

func (c *Cache) Len() int64 {
	return int64(c.ll.Len())
}
~~~

使用了标准库中原生的 `list.List`——一种双端队列数据结构，其中每个节点的数据结构是 `list.Element`，其中元素中存放的值 Value 在上述 Cache 中是：entry 类型，是一种 `key-value` 值。

对应的测试代码：

~~~go
package lru

import (
	"reflect"
	"testing"
)

type String string

func (s String) Len() int {
	return len(s)
}

func TestGet(t *testing.T) {
	lru := New(int64(0), nil)
	lru.Add("key1", String("1234"))

	if v, ok := lru.Get("key1"); !ok || string(v.(String)) != "1234" {
		t.Log(ok, string(v.(String)))
		t.Fatal("cache hit key1=1234 failed")
	}
	if _, ok := lru.Get("key2"); ok {
		t.Fatal("cache miss key2 failed")
	}
}

func TestRemoveoldest(t *testing.T) {
	k1, k2, k3 := "key1", "key2", "key3"
	v1, v2, v3 := "value1", "value2", "value3"

	cap := len(k1 + k2 + v1 + v2)
	lru := New(int64(cap), nil)
	lru.Add(k1, String(v1))
	lru.Add(k2, String(v2))
	lru.Add(k3, String(v3))

	if _, ok := lru.Get("key1"); ok || lru.Len() != 2 {
		t.Fatal("Removeoldest key1 failed")
	}
}

func TestOnEvicted(t *testing.T) {
	keys := make([]string, 0)
	callback := func(key string, value Value) {
		keys = append(keys, key)
	}

	lru := New(int64(10), callback)
	lru.Add("key1", String("123456"))
	lru.Add("k2", String("k2"))
	lru.Add("k3", String("k3"))
	lru.Add("k4", String("k4"))

	expect := []string{"key1", "k2"}

	t.Log(keys)
	if !reflect.DeepEqual(expect, keys) {
		t.Fatalf("call onRemoved failed, expect keys equals to %s", expect)
	}
}
~~~

# 2 单机并发缓存

想要了解并发，可以从简单的例子出发：

~~~go
package main

import (
	"fmt"
	"time"
)

func main() {
	set := make(map[int]bool)

	for i := 0; i < 10; i++ {
		go func() {
			if _, ok := set[100]; !ok {
				fmt.Println("100")
				set[100] = true
			}
		}()
	}

	time.Sleep(1 * time.Second)
}
~~~

这段程序的核心数据变量是：set，是一个 `map[int]bool`。我们知道在 Go 中的 map 是不支持并发访问的，这段程序的输出有很多中形式，可能打印了 4 个 100，也可能打印了很多个，还有可能直接 crash——`fatal error: concurrent map read and map write`！

多个 goroutine **同时读写同一个变量**，在并发度较高的情况下，会发生**冲突**。确保一次只有一个 goroutine 可以访问该变量以避免冲突，这称之为`互斥`，互斥锁可以解决这个问题。

> sync.Mutex 是一个互斥锁，可以由**不同的** goroutine 加锁和解锁。

`sync.Mutex` 是 Go 语言标准库提供的一个互斥锁，当一个 goroutine 获得了这个锁的拥有权后，其它**请求锁**的 goroutine 就会阻塞在 `Lock()` 方法的调用上，直到调用 `Unlock()` 锁被释放。

经过 sync.Mutex 改造后的代码如下：

~~~go
package main

import (
	"fmt"
	"sync"
	"time"
)

func main() {
	set := make(map[int]bool)

	var lock sync.Mutex

	for i := 0; i < 10; i++ {
		go func() {
			lock.Lock()
			defer lock.Unlock()
			if _, ok := set[100]; !ok {
				fmt.Println("100")
				set[100] = true
			}
		}()
	}

	time.Sleep(1 * time.Second)
}
~~~

相同的数字只会被打印一次。当一个 goroutine 调用了 `Lock()` 方法时，**其他 goroutine 调用 `Lock()` 时会被阻塞**，直到 `Unlock()` 调用将锁释放。因此被包裹部分的代码就能够避免冲突，实现互斥。

在将支持并发的功能集成到项目前，封装表示缓存值的数据结构 ByteView：

~~~go
package v4

type ByteView struct {
	b []byte
}

// Len 实现 lru.go 中的 Value 接口
func (v ByteView) Len() int {
	return len(v.b)
}

// ByteSlice 做了一次深拷贝，防止反馈给用户后修改缓存值（Update入口做统一管理）
func (v ByteView) ByteSlice() []byte {
	return cloneBytes(v.b)
}

func (v ByteView) String() string {
	return string(v.b)
}

func cloneBytes(src []byte) []byte {
	//FIXME dest并没有初始化 make([]byte, len(v.b))，如果没有初始化会出现问题
	var dest []byte
	dest = make([]byte, len(src))
	copy(dest, src)
	return dest
}
~~~

ByteView 是一个**只读的数据结构**，用来表示缓存值。关于这个 ByteView 的设计主要考虑：

1. ByteView 的方法接收者是 ByteView 类型，而不是其指针类型，避免缓存值被修改；
2. ByteSlice 做了深拷贝，同样是为了避免修改；
3. ByteView 只有一个数据成员，`b []byte`，b 将会**存储真实的缓存值**。选择 byte 类型是为了能够支持任意的数据类型的存储，例如字符串、图片等。

接下来为 Cache 添加并发特性：

~~~go
package v4

import (
	"sync"

	"github.com/go-examples-with-tests/net/http/v4/lru"
)

type cache struct {
	lock       sync.Mutex // 无需初始化，直接就可使用
	lru        *lru.Cache
	cacheBytes int64
}

func (c *cache) add(key string, view ByteView) {
	c.lock.Lock()
	defer c.lock.Unlock()

	if c.lru == nil {
		c.lru = lru.New(c.cacheBytes, nil)
	}
	c.lru.Add(key, view)
}

func (c *cache) get(key string) (v ByteView, ok bool) {
	c.lock.Lock()
	defer c.lock.Unlock()
	if c.lru == nil {
		return
	}

	if value, ok := c.lru.Get(key); ok {
		return value.(ByteView), true // 返回了 ByteView 后，不会被改变吗？
	}
	return
}

~~~

在 `add` 方法中，判断了 `c.lru` 是否为 nil，如果等于 nil 再创建实例。这种方法称之为**延迟初始化**(Lazy Initialization)，一个对象的延迟初始化意味着该对象的创建将会延迟至第一次使用该对象时。这种做法，主要**用于提高性能，并减少程序内存要求**。

我们可以看到，cache 实际上进一步封装了 `*lur.Cache`，同时初始化了一个 sync.Mutex 实例用于并发访问。这种做法和工程基建类似，都是从“打地基”开始，一步步向上构建——**自底向上**。

接下来我们构建分布式缓存系统的**逻辑雏形**：

~~~go
                            是
接收 key --> 检查是否被缓存 -----> 返回缓存值 ⑴
                |  否                         是
                |-----> 是否应当从远程节点获取 -----> 与远程节点交互 --> 返回缓存值 ⑵
                            |  否
                            |-----> 调用`回调函数`，获取值并添加到缓存 --> 返回缓存值 ⑶
~~~

上面的步骤 1 和 3 是本节内容实现的部分，步骤 2 需要**让缓存系统支持分布式**。

整个代码结构如下：

~~~go
geecache/
    |--lru/
        |--lru.go  // lru 缓存淘汰策略
    |--byteview.go // 缓存值的抽象与封装
    |--cache.go    // 并发控制
    |--geecache.go // 负责与外部交互，控制缓存存储和获取的主流程
~~~

geocache.go 中封装的 GeeCache 数据结构是整个分布式缓存系统的**核心数据结构**，负责**与外部交互，并控制缓存存储和获取的主流程**。下面来实现主要逻辑：

~~~go
package v4

import (
	"fmt"
	"log"
	"sync"
)

type Group struct {
	name      string
	getter    Getter
	mainCache cache //FIXME 此处为什么不能是 *cache？什么时候使用指针，什么时候使用普通类型？
}

var (
	mu     sync.RWMutex
	groups = make(map[string]*Group) //FIXME 此处为什么存的是 *Group？
)

type Getter interface {
	Get(key string) ([]byte, error)
}

type GetterFunc func(key string) ([]byte, error) // 接口型函数，实现了Getter接口

func (f GetterFunc) Get(key string) ([]byte, error) {
	return f(key)
}

func NewGroup(name string, cacheBytes int64, getter Getter) *Group {
	if getter == nil {
		panic("getter is nil")
	}

	if _, exist := groups[name]; exist {
		panic("group " + name + " exists")
	}

	mu.Lock()
	defer mu.Unlock()
	g := &Group{
		name:      name,
		getter:    getter,
		mainCache: cache{cacheBytes: cacheBytes},
	}
	groups[name] = g
	return g
}

func GetGroup(name string) *Group {
    // 使用了只读锁 RLock()，因为不涉及任何冲突变量的写操作。
	mu.RLock()
	// defer mu.RUnlock()
	g := groups[name]
	mu.RUnlock()
	return g
}

func (g *Group) Get(key string) (ByteView, error) {
	if key == "" {
		return ByteView{}, fmt.Errorf("key is required")
	}
	if v, ok := g.mainCache.get(key); ok {
		log.Println("[GeeCache] hit")
		return v, nil
	}

	return g.load(key)
}

func (g *Group) load(key string) (ByteView, error) {
	// 调用 getter，用户自定义获取数据方式
	return g.getLocally(key)
}

func (g *Group) getLocally(key string) (ByteView, error) {
	// 调用 getter 从数据源获取数据
	bytes, err := g.getter.Get(key)
	if err != nil {
		return ByteView{}, err
	}
	value := ByteView{b: cloneBytes(bytes)}
	g.populateCache(key, value)
	return value, nil
}

func (g *Group) populateCache(key string, value ByteView) {
	g.mainCache.add(key, value)
}
~~~

如果缓存不存在，应从数据源（文件，数据库等）获取数据并添加到缓存中。GeeCache 是否应该支持多种数据源的配置呢？不应该，一是数据源的种类太多，没办法一一实现；二是扩展性不好。如何从源头获取数据，应该是用户决定的事情，我们就把这件事交给用户好了。因此，我们设计了一个**回调函数**(callback)，在缓存不存在时，调用这个函数，得到源数据。

- 定义接口 Getter 和 回调函数 `Get(key string)([]byte, error)`，参数是 key，返回值是 []byte。
- 定义函数类型 GetterFunc，并实现 Getter 接口的 `Get` 方法。
- 函数类型实现某一个接口，称之为**接口型函数**，方便使用者在调用时既能够传入函数作为参数，也能够传入实现了该接口的结构体作为参数。

定义一个函数类型 F，并且实现接口 A 的方法，然后在这个方法中调用自己。这是 Go 语言中将其他函数（参数返回值定义与 F 一致）转换为接口 A 的常用技巧。

代码测试：

~~~go
package v4

import (
	"fmt"
	"log"
	"reflect"
	"testing"
)

var db = map[string]string{
	"Tom":  "630",
	"Jack": "589",
	"Sam":  "567",
}

func TestGetter(t *testing.T) {
	var f Getter = GetterFunc(func(key string) ([]byte, error) { // 函数类型的初始化，并赋值给一个接口
		return []byte(key), nil
	})
	expect := []byte("key")
	if v, _ := f.Get("key"); !reflect.DeepEqual(v, expect) {
		t.Fatal("callback failed")
	}
}

func TestGroup(t *testing.T) {
	NewGroup("score", 2<<10, GetterFunc(func(key string) (bytes []byte, err error) {
		return
	}))

	if group := GetGroup("score"); group == nil || group.name != "score" {
		t.Fatal("create new group failed")
	}
	if group := GetGroup("score" + "xxx"); group != nil {
		t.Fatal("get a error group")
	}
}

func TestGet(t *testing.T) {
	loadCount := make(map[string]int)
	gee := NewGroup("school score", 2<<10, GetterFunc(func(key string) ([]byte, error) {
		log.Println("[SlowDB] search key:", key)
		if v, ok := db[key]; ok {
			if _, ok := loadCount[key]; !ok {
				loadCount[key] = 0
			}
			// 表示从db中加载数据的次数
			loadCount[key]++
			return []byte(v), nil
		}
		return nil, fmt.Errorf("%s not exist", key)
	}))
	for k, v := range db {
		if view, err := gee.Get(k); err != nil || view.String() != v {
			t.Log(err, view.String())
			t.Fatalf("failed to get value of %s", k)
		}
		if _, err := gee.Get(k); err != nil || loadCount[k] > 1 {
			t.Fatalf("cache %s miss", k)
		}
	}
	if view, err := gee.Get("unknown"); err == nil {
		t.Fatalf("the value of unknow should be empty, but %s got", view)
	}
}
~~~

# 3 HTTP 服务端

分布式缓存需要实现节点间通信，建立基于 HTTP  通信机制是比较常见和简单的做法。如果一个节点启动了 HTTP 服务，那么这个节点就可以被其他节点访问。下面的内容就是为单机节点搭建 HTTP Server：

~~~go
geecache/
    |--lru/
        |--lru.go  // lru 缓存淘汰策略
    |--byteview.go // 缓存值的抽象与封装
    |--cache.go    // 并发控制
    |--geecache.go // 负责与外部交互，控制缓存存储和获取的主流程
	|--http.go     // 提供被其他节点访问的能力(基于http)
~~~

创建一个结构体 HTTPPool，作为承载节点间 HTTP 通信的核心数据结构：

~~~go
const defaultBasePath = "/_geecache/"

// 承载节点 HTTP 通信的核心数据结构
type HTTPPool struct {
	self     string
	basePath string
}

func NewHTTPPool(self string) *HTTPPool {
	return &HTTPPool{
		self:     self,
		basePath: defaultBasePath,
	}
}
~~~

- `HTTPPool` 只有 2 个参数，一个是 self，用来记录自己的地址，包括主机名/IP 和端口。
- 另一个是 basePath，作为节点间通讯地址的前缀，默认是 `/_geecache/`，那么 http://example.com/_geecache/ 开头的请求，就用于节点间的访问。因为一个主机上还可能承载其他的服务，加一段 Path 是一个好习惯。比如，大部分网站的 API 接口，一般以 `/api` 作为前缀。

实现**访问缓存**的方法：

~~~go
func (p *HTTPPool) Log(format string, v ...interface{}) {
	log.Printf("[Server %s] %s", p.self, fmt.Sprintf(format, v...))
}

func (p *HTTPPool) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	if !strings.HasPrefix(r.URL.Path, p.basePath) {
		panic("HTTPPool serving unexpected path: " + r.URL.Path)
	}
	p.Log("%s %s", r.Method, r.URL.Path)

	// /<basepath>/<groupname>/<key>
	parts := strings.SplitN(r.URL.Path[len(p.basePath):], "/", 2)
	if len(parts) != 2 {
		http.Error(w, "bad request", http.StatusBadRequest)
		return
	}

	groupName := parts[0]
	key := parts[1]
	group := GetGroup(groupName)
	if group == nil {
		http.Error(w, "no such group:"+groupName, http.StatusNotFound)
		return
	}

	view, err := group.Get(key)
	if err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}
	w.Header().Set("Content-Type", "application/octet-stream")
	w.Write(view.ByteSlice())
	w.Write([]byte("\r\n"))
}
~~~

接下来测试：

~~~go
package main

import (
	"fmt"
	"log"
	"net/http"

	v4 "github.com/go-examples-with-tests/net/http/v4"
)

func main() {
	var db = map[string]string{
		"Tom":  "630",
		"Jack": "589",
		"Sam":  "567",
	}

	v4.NewGroup("scores", 2<<10, v4.GetterFunc(func(key string) ([]byte, error) {
		log.Println("[SlowDB] search key ", key)
		if v, ok := db[key]; ok {
			return []byte(v), nil
		}
		return nil, fmt.Errorf("%s not exist", key)
	}))

	addr := ":9999"
	peers := v4.NewHTTPPool(addr)
	log.Println("geecache is running at ", addr)
	log.Fatal(http.ListenAndServe(addr, peers))
}
~~~

上面单机已经启动了 HTTP 服务：

~~~shell
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Sam
567
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Jack
589
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Tom 
630
ant@MacBook-Pro ~ % curl http://localhost:9999/_geecache/scores/Katyusha
Katyusha not exist
~~~

如果多次访问某个学员的分数情况，比如 Jack，由于缓存系统已经存储了，不再去 DB 中查找，直接从缓存中取。

节点间的相互通信不仅需要 HTTP 服务端，还需要 HTTP 客户端，这就是我们下一步需要做的事情。

# 4 一致性哈希 Hash

**一致性哈希算法**是缓存系统从**单节点**走向**分布式节点**的一个重要的环节！

对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？假设包括一个有 10 个节点，第一次随机选取了节点1， 节点1从**数据源**获取到数据的同时**缓存该数据**；那第二次只有 1/10 的可能性再次选择节点1，有9/10的概率选择了其他节点，如果选择了其他节点，就意味着需要**再次**从数据源获取数据，一般来说，这个操作是耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

有什么办法对于**给定的 key**，每一次都选择同一个节点？使用 hash 算法，**计算出 key 的哈希值**，再除以整个节点数量取余数，得到的就是节点索引。这样做，相当于是**自定义哈希算法**得到了对应单机缓存系统的位置索引，也就是**找到了当前缓存存储的位置**：

![](./img/hash_select.png)

从上面的图可以看到，任意一个节点任意时刻请求查找键 `Tom` 对应的值，都会分配给节点 2，有效地解决了上述的问题。

简单求取哈希值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设某种场景中移除了一台节点，只剩下 9 个单节点的分布式缓存系统，那么之前的 `hash(key)%10` 变成了 `hash(key)%9`，也就意味着几乎缓存值对应的节点都发生了改变，即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新取数据源获取数据，容易引起**缓存雪崩**——**缓存在同一时刻全部失效**，造成瞬时 DB 请求量大、压力骤增，引起雪崩。常因为缓存服务器宕机，或缓存设置了相同的过期时间引起的。

一致性哈希算法就是为了解决这个问题！一致性哈希算法将 key **映射（这个映射的过程就是计算哈希值的过程）**到 2^32^ 的空间中，将**这个数字**首尾相连，形成一个环：

* 计算节点/机器（通常使用节点的名称、编号和 IP 地址）的**哈希值**，放置在环上；
* 计算 key 的**哈希值**，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

![](./img/add_peer.png)

环上有 peer2，peer4，peer6 三个节点，`key11`，`key2`，`key27` 均映射到 peer2，`key23` 映射到 peer4。此时，如果新增节点/机器 peer8，假设它新增位置如图所示，那么只有 `key27` 从  peer2 调整到  peer8，**其余的映射均没有发生改变**。也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的**一小部分数据**，而不需要重新定位**所有的节点**，这就解决了上述的问题。这一小部分数据仍然需要从数据源获取数据！

如果服务器的节点过少，容易**引起 key 的倾斜**。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，**缓存节点间负载不均**。为了解决这个问题，引入了**虚拟节点**的概念，一个真实节点对应多个虚拟节点。假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是  peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

- 第一步，计算虚拟节点的 Hash 值，放置在环上。
- 第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

一致性哈希算法的实现：

~~~go
package v4

import (
	"hash/crc32"
	"sort"
	"strconv"
)

type Hash func(data []byte) uint32

type Map struct {
	hash     Hash
	replicas int            // 扩充虚拟节点
	keys     []int          // 所有节点根据 hash 值排序（0~2^31-1）
	hashMap  map[int]string // 节点 hash 值和节点名称的映射
}

func New(replicas int, fn Hash) *Map {
	m := &Map{
		replicas: replicas,
		hash:     fn,
		hashMap:  make(map[int]string),
	}
	if m.hash == nil {
		m.hash = crc32.ChecksumIEEE
	}
	return m
}

// 向 Map 中新增节点
func (m *Map) Add(keys ...string) {
	for _, key := range keys {
		// 一个缓存节点扩展成 m.replicas 个节点，相当于增加了虚拟节点
		for i := 0; i < m.replicas; i++ {
			hash := int(m.hash([]byte(strconv.Itoa(i) + key)))
			m.keys = append(m.keys, hash)
			m.hashMap[hash] = key // key 可认为是某个节点，也就是对一个的某个缓存系统
		}
	}
	// 让所有节点按照 hash 的大小依次顺序排列在整个 0 ~ 2^31-1 组成的环上
	sort.Ints(m.keys)
}

func (m *Map) Get(key string) string {
	if len(m.keys) == 0 {
		return "" // 表示没有匹配到任何缓存系统节点
	}
	hash := int(m.hash([]byte(key)))
	idx := sort.Search(len(m.keys), func(i int) bool {
		return m.keys[i] >= hash
	})
	// circle index
	return m.hashMap[m.keys[idx%len(m.keys)]]
}
~~~

测试用例：

~~~go
package v4

import (
	"log"
	"strconv"
	"testing"
)

func TestHash(t *testing.T) {
	hash := New(3, func(data []byte) uint32 {
		// "06" --> 6; "16" --> 16
		i, _ := strconv.Atoi(string(data))
		return uint32(i)
	})

	hash.Add("6", "4", "2")

	testCases := map[string]string{
		"2":  "2",
		"11": "2",
		"23": "4",
		"27": "2",
	}

	for k, v := range testCases {
		got := hash.Get(k)
		log.Printf("ask for %s, got: %s", k, got)
		if got != v {
			t.Fatalf("Asking for %s, should have yielded %s", k, v)
		}
	}

	hash.Add("8")
	testCases["27"] = "8"
	for k, v := range testCases {
		got := hash.Get(k)
		log.Printf("ask for %s, got: %s", k, got)
		if got != v {
			t.Fatalf("Asking for %s, should have yielded %s", k, v)
		}
	}
}
~~~

# 5 分布式节点















# 6 防止缓存击穿

















# 7 使用 Protobuf 通信